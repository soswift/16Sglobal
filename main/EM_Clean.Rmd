---
title: "Global 16S - R Notebook"
output: html_notebook
---
# Summary
This notebook is working wtih qiita outputs (qiime2) for 16S data.
In practice, this means importing biom format (HDF5) files into R, which can be resource intensive and tricky.

There are two data sets available:
  - Waimea 16S data
      This data only includes samples from waimea valley and was processed following EMP guidelines
  - Global EMP 16S data
      This data includes the full EMP 90bp dataset, along with waimea samples

## Data cleaning

### Subset OTUs
Since we are focused on comparing waimea samples to the global data set, we can cut down the ESVs to just those present in Waimea
First, importthe waimea dataset and write out the list of ESVs to subset to.

```{r}
library(biomformat)
library(dplyr)
library(data.table)

# read in waimea 16S biom
wai_16S <- read_hdf5_biom("../data/biom/94580_reference-hit.biom")

# pull out feature ids
esv_ids <-unique(rapply(wai_16S$rows, function(x) head(x, 1)))

# write out as text file
writeLines(esv_ids, "../data/processed/esvs_to_keep.txt")

```

Next, subset the esvs using a qiime1 script. 
By using the qiime script, we can avoid reading the whole EMP dataset into memory and can run on the HPC

The script is called filter_otus_from_otu_table.py: http://qiime.org/scripts/filter_otus_from_otu_table.html

```{bash eval=FAlSE }

filter_otus_from_otu_table.py -i [input file] -o [output file] -e [esv list] --negate_ids_to_exclude

```

### Subset Taxonomy To Match
Now that we have fewer OTUs, we also need a matching taxonomy file.
The taxonomy file from Qiita can be downloaded as a TSV or included in the BIOM file.

Since it's easier to work with, let's just use the TSV file.

```{r}
full_taxonomy <- fread("../data/biom/94706_taxonomy.tsv")
esv_ids <- fread("../data/processed/esvs_to_keep.txt", header = F)
keep <- full_taxonomy$`Feature ID` %in% esv_ids$V1


wai_taxonomy  <- full_taxonomy[keep]

fwrite(wai_taxonomy, "../data/processed/global_16S_subset_tax.tsv",  sep = "\t")

```




### Deal With Problematic Metadata
The metadata associated with all 30,000 EMP samples can be quite messy.
When trying to read it into excel or R, we hit issues with end of line ("\n") characters inside of strings.

This also causes problems when trying to read the full biom file into R.
Since the otu table, taxonomy, and metadat are joined, if metadata fails to read, they all do.

One option for manipulating biom files is python.
I'm not a python pro, but here's how you would do it using the biom-format module.

The biom-format module seems to use less memory than R's biomformat package.
We can either read the file in using a python script or use the command line to convert to a tsv file.

Reading the biom table into Python:

The biom.table object has the following methods available:
https://biom-format.org/documentation/generated/biom.table.Table.html

```{pyton eval=FALSE}
from biom import load_table
table = load_table('global16S_subset.biom')

table.head

```

From there, presumably you could subset metadata columns and write it all out as a biom file again.
Beats me!


There's also a package in R for dealing with hdf5

```{r}
library(rhdf5)

biom_file <- "../data/biom/global16S_subset.biom"

h5ls(biom_file)
biom_sub <- H5Fopen(biom_file)
biom_sub

biom_sub$observation$metadata$

h5delete()

```



#### Using Bash
The alternative is to try to do everything with bash and command line programs.

Heads up, this will take a long time  and possibly run out of memory for a larg-ish biom file.
Ours is 125mb with ~300,000 columns. 

I can download the problematic metadata file separately from Qiita.
Therefore, my goal is to strip the metadata out off the biom file so we can read it into R without problems.

To start, I tried writing out the otu table as a tsv file.
Converting just the otu table to tsv required HPC resources (8 cpus, 100gb ram).
The final, uncompressed, tsv was 32G. Compressed (gzip), only 89 M, which is better for moving around. 

Here's the script I submitted to slurm

##### Converting OTU table from BIOM to TSV

```{bash eval=FALSE}
# command line call, with biom-format python package installed

source ~/.bash_profile # Not required unless you need something from your environment

module load lang/Python/3.7.2-intel-2018.5.274

cd /home/seanos/cmaiki-lts/seanos/waimea/biom/subset_global16S

# pull out otu table
biom convert -i global16S_subset.biom -o global16S_subset_temp.tsv --to-tsv


```

##### Cleaning Metadata TSV
At this point, we should have a clean biom file with otus.
We still need to clean up the metadata though! When converting to tsv, you can specify metadata columns that you want to write out separately.
However, we have a separate metadata file and taxonomy file anyways so there's no need.
Next, I'll clean up the metadata, which has hundreds of columns from all the different studies.
We only need a few of these columns for our study.

We can't read the metadata into R or excel because some of the columns are corrupted (e.g. end of line characters in strings ('\n').
However, bash has some useful tools that seem to avoid these problems.

```{bash eval=F}

# cleaning global 16S metadata
# I think there is corruption in the metadata. 
# We don't need all these columns, so try cutting out the ones we don't need.

# 1) pull out column names from global dataset and arrange one on each line with line number pre-pended
head -n 1 30272_30272_analysis_mapping.txt | sed 's/\t/\n/g' | nl > global_cols.txt


# 2) pull out column names from waimea dataset
head -n 1 13115_prep_9095_qiime_20200618-140936.txt | sed 's/\t/\n/g' > waimea_cols.txt

# 3) find exact name matches using the join program

join -1 2 -2 1 <(sort -k 2 global_cols.txt) <(sort waimea_cols.txt)  > col_key.txt

# 4) print out comma separated string of line numbers to pass to cut
cols=$(sed -e 's/.*\s//g' col_key.txt | tr '\n' ',')

# cols =
# 1,2,3,162,165,168,172,278,279,280,285,286,288,292,293,314,320,356,364,
# 366,381,407,442,445,463,479,483,493,511,590,731,738,770,793,817,820,822,855,889,968,1040

# 5) cut those columns out of the tsv file
cut -f "$cols global16S_mapping_file.tsv > clean_map_global16S.tsv


```













```











